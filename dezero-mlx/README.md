# ğŸ DeZero-MLX

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.8+-blue.svg" alt="Python">
  <img src="https://img.shields.io/badge/Platform-macOS-lightgrey.svg" alt="macOS">
  <img src="https://img.shields.io/badge/Backend-MLX%20%7C%20NumPy-orange.svg" alt="Backend">
  <img src="https://img.shields.io/badge/License-MIT-green.svg" alt="License">
</p>

<p align="center">
  <b>Deep Learning Framework from Scratch with Apple Silicon GPU Support</b>
</p>

<p align="center">
  <a href="#-features">Features</a> â€¢
  <a href="#-installation">Installation</a> â€¢
  <a href="#-quick-start">Quick Start</a> â€¢
  <a href="#-examples">Examples</a> â€¢
  <a href="#í•œêµ­ì–´">í•œêµ­ì–´</a>
</p>

---

**DeZero-MLX** is a deep learning framework built from scratch, modified to support Apple Silicon GPUs via [MLX](https://github.com/ml-explore/mlx). Based on the original [DeZero](https://github.com/oreilly-japan/deep-learning-from-scratch-3) framework from the book "Deep Learning from Scratch 3".

## âœ¨ Features

- ğŸš€ **Apple Silicon GPU Acceleration** - Seamless MLX backend support
- ğŸ”„ **Automatic Differentiation** - Define-by-run dynamic computation graph
- ğŸ§  **Neural Network Layers** - Linear, Conv2d, LSTM, BatchNorm, and more
- âš¡ **Optimizers** - SGD, Momentum, AdaGrad, Adam
- ğŸ“Š **Built-in Datasets** - MNIST, Spiral, SinCurve
- ğŸ”§ **NumPy Fallback** - Works on any platform without MLX

## ğŸ“¦ Installation

### Requirements

- Python 3.8+
- NumPy
- (Optional) MLX for Apple Silicon GPU support

### Install

```bash
# Clone the repository
git clone https://github.com/yourusername/dezero-mlx.git
cd dezero-mlx

# Install dependencies
pip install numpy

# (Optional) Install MLX for GPU acceleration on Apple Silicon
pip install mlx
```

## ğŸš€ Quick Start

```python
import numpy as np
from dezero import Variable
import dezero.functions as F

# Automatic differentiation
x = Variable(np.array(2.0))
y = x ** 2 + 3 * x + 1
y.backward()

print(f"y = {y.data}")      # y = 11.0
print(f"dy/dx = {x.grad}")  # dy/dx = 7.0
```

## ğŸ“š Examples

### Neural Network Training

```python
from dezero import Variable, Model
from dezero import optimizers
import dezero.functions as F
import dezero.layers as L

# Define model
class MLP(Model):
    def __init__(self, hidden_size, out_size):
        super().__init__()
        self.l1 = L.Linear(hidden_size)
        self.l2 = L.Linear(out_size)
    
    def forward(self, x):
        x = F.relu(self.l1(x))
        return self.l2(x)

# Train
model = MLP(100, 10)
optimizer = optimizers.Adam().setup(model)

for epoch in range(100):
    y = model(x)
    loss = F.softmax_cross_entropy(y, t)
    
    model.cleargrads()
    loss.backward()
    optimizer.update()
```

### LSTM Time Series Prediction

```python
from dezero.layers import LSTM, Linear
from dezero.models import Model

class LSTMPredictor(Model):
    def __init__(self, hidden_size):
        super().__init__()
        self.lstm = LSTM(hidden_size)
        self.fc = Linear(1)
    
    def reset_state(self):
        self.lstm.reset_state()
    
    def forward(self, x):
        h = self.lstm(x)
        return self.fc(h)
```

### Transformer (Self-Attention)

```python
def scaled_dot_product_attention(Q, K, V):
    d_k = Q.shape[-1]
    scores = F.matmul(Q, K.transpose(0, 2, 1)) / np.sqrt(d_k)
    attn_weights = F.softmax(scores, axis=-1)
    return F.matmul(attn_weights, V), attn_weights

class MultiHeadAttention(Layer):
    def __init__(self, d_model, n_heads):
        super().__init__()
        self.W_q = L.Linear(d_model)
        self.W_k = L.Linear(d_model)
        self.W_v = L.Linear(d_model)
        self.W_o = L.Linear(d_model)
    
    def forward(self, x):
        Q, K, V = self.W_q(x), self.W_k(x), self.W_v(x)
        out, _ = scaled_dot_product_attention(Q, K, V)
        return self.W_o(out)
```

### GPU Acceleration (MLX)

```python
from dezero import cuda

# Check GPU availability
print(f"MLX available: {cuda.gpu_enable}")

# Move data to GPU
x_gpu = cuda.as_gpu(x_numpy)

# Move back to CPU
x_cpu = cuda.as_numpy(x_gpu)
```

## ğŸ“ Project Structure

```
dezero/
â”œâ”€â”€ __init__.py          # Package initialization
â”œâ”€â”€ core.py              # Variable, Function, Parameter
â”œâ”€â”€ cuda.py              # MLX/NumPy backend switching
â”œâ”€â”€ functions.py         # Activation, loss functions
â”œâ”€â”€ functions_conv.py    # Convolution operations
â”œâ”€â”€ layers.py            # Linear, Conv2d, LSTM, etc.
â”œâ”€â”€ models.py            # Model, Sequential, MLP, VGG, ResNet
â”œâ”€â”€ optimizers.py        # SGD, Adam, etc.
â”œâ”€â”€ datasets.py          # MNIST, Spiral, etc.
â”œâ”€â”€ dataloaders.py       # DataLoader, SeqDataLoader
â”œâ”€â”€ transforms.py        # Image transforms
â””â”€â”€ utils.py             # Utilities
```

## ğŸ”§ Supported Features

### Layers
| Layer | Description |
|-------|-------------|
| `Linear` | Fully connected layer |
| `Conv2d` | 2D convolution |
| `Deconv2d` | 2D transposed convolution |
| `LSTM` | Long Short-Term Memory |
| `RNN` | Recurrent Neural Network |
| `BatchNorm` | Batch Normalization |
| `EmbedID` | Embedding layer |

### Functions
| Function | Description |
|----------|-------------|
| `relu`, `sigmoid`, `tanh` | Activation functions |
| `softmax`, `log_softmax` | Softmax functions |
| `softmax_cross_entropy` | Cross entropy loss |
| `mean_squared_error` | MSE loss |
| `dropout` | Dropout regularization |
| `conv2d`, `pooling` | Convolution operations |

### Optimizers
| Optimizer | Description |
|-----------|-------------|
| `SGD` | Stochastic Gradient Descent |
| `MomentumSGD` | SGD with momentum |
| `AdaGrad` | Adaptive gradient |
| `Adam` | Adaptive moment estimation |

## ğŸ§ª Running Tests

```bash
python test_dezero.py
```

## ğŸ“– References

- [Deep Learning from Scratch 3](https://www.oreilly.co.jp/books/9784873119069/) - Original DeZero
- [MLX Documentation](https://ml-explore.github.io/mlx/) - Apple's ML framework

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Original DeZero by [Koki Saitoh](https://github.com/oreilly-japan/deep-learning-from-scratch-3)
- MLX by [Apple](https://github.com/ml-explore/mlx)

---

<a name="í•œêµ­ì–´"></a>
# ğŸ DeZero-MLX (í•œêµ­ì–´)

**ë°‘ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ - Apple Silicon GPU ì§€ì›**

## âœ¨ íŠ¹ì§•

- ğŸš€ **Apple Silicon GPU ê°€ì†** - MLX ë°±ì—”ë“œ ì§€ì›
- ğŸ”„ **ìë™ ë¯¸ë¶„** - Define-by-run ë™ì  ê³„ì‚° ê·¸ë˜í”„
- ğŸ§  **ì‹ ê²½ë§ ë ˆì´ì–´** - Linear, Conv2d, LSTM, BatchNorm ë“±
- âš¡ **ì˜µí‹°ë§ˆì´ì €** - SGD, Momentum, AdaGrad, Adam
- ğŸ“Š **ë‚´ì¥ ë°ì´í„°ì…‹** - MNIST, Spiral, SinCurve
- ğŸ”§ **NumPy í´ë°±** - MLX ì—†ì´ë„ ëª¨ë“  í”Œë«í¼ì—ì„œ ì‘ë™

## ğŸ“¦ ì„¤ì¹˜

```bash
# ì €ì¥ì†Œ í´ë¡ 
git clone https://github.com/yourusername/dezero-mlx.git
cd dezero-mlx

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install numpy

# (ì„ íƒ) Apple Silicon GPU ê°€ì†ì„ ìœ„í•œ MLX ì„¤ì¹˜
pip install mlx
```

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

```python
import numpy as np
from dezero import Variable

# ìë™ ë¯¸ë¶„
x = Variable(np.array(2.0))
y = x ** 2 + 3 * x + 1
y.backward()

print(f"y = {y.data}")       # y = 11.0
print(f"dy/dx = {x.grad}")   # dy/dx = 7.0
```

## ğŸ“š ì˜ˆì œ

ìì„¸í•œ ì˜ˆì œëŠ” ë‹¤ìŒ íŒŒì¼ë“¤ì„ ì°¸ê³ í•˜ì„¸ìš”:
- `test_dezero_full.py` - ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
- `test_advanced_models.py` - LSTM, Transformer í…ŒìŠ¤íŠ¸

## ğŸ“– ì°¸ê³ ìë£Œ

- [ë°‘ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹ 3](https://www.hanbit.co.kr/store/books/look.php?p_code=B6627606922) - ì›ë³¸ DeZero
- [MLX ë¬¸ì„œ](https://ml-explore.github.io/mlx/) - Apple ML í”„ë ˆì„ì›Œí¬
