#!/usr/bin/env python3
"""
DeZero ì¢…í•© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
macOS MLX ì§€ì› ë²„ì „
"""
import sys
sys.path.append('/Users/dbwns/í”„ë¡œê·¸ë˜ë°/DeepLearning')

import numpy as np

# =============================================================================
# 1. ê¸°ë³¸ Import í…ŒìŠ¤íŠ¸
# =============================================================================
print("=" * 60)
print("ğŸ DeZero macOS MLX ë²„ì „ í…ŒìŠ¤íŠ¸")
print("=" * 60)

import dezero
from dezero import Variable, Parameter
from dezero import functions as F
from dezero import layers as L
from dezero import optimizers
from dezero import cuda
from dezero.models import MLP, Sequential
from dezero.datasets import Spiral
from dezero.dataloaders import DataLoader

print(f"\nâœ“ DeZero v{dezero.__version__} ë¡œë“œ ì™„ë£Œ")
print(f"âœ“ GPU(MLX) ì‚¬ìš© ê°€ëŠ¥: {cuda.gpu_enable}")

# =============================================================================
# 2. ìë™ ë¯¸ë¶„ í…ŒìŠ¤íŠ¸
# =============================================================================
print("\n" + "=" * 60)
print("ğŸ“ ìë™ ë¯¸ë¶„ í…ŒìŠ¤íŠ¸")
print("=" * 60)

# ê°„ë‹¨í•œ í•¨ìˆ˜: y = x^2 + 2x + 1
x = Variable(np.array(3.0))
y = x ** 2 + 2 * x + 1
y.backward()

print(f"\ny = xÂ² + 2x + 1")
print(f"x = {x.data}")
print(f"y = {y.data}")
print(f"dy/dx = {x.grad.data} (ì •ë‹µ: 2*3+2 = 8)")

# ë³µì¡í•œ í•¨ìˆ˜: z = sin(x) + cos(x)
x = Variable(np.array(np.pi / 4))
z = F.sin(x) + F.cos(x)
z.backward()

print(f"\nz = sin(x) + cos(x)")
print(f"x = Ï€/4 = {x.data:.4f}")
print(f"z = {z.data:.4f} (ì •ë‹µ: âˆš2 â‰ˆ 1.414)")
print(f"dz/dx = {x.grad.data:.4f} (ì •ë‹µ: cos(Ï€/4) - sin(Ï€/4) = 0)")

# =============================================================================
# 3. ì‹ ê²½ë§ ë ˆì´ì–´ í…ŒìŠ¤íŠ¸
# =============================================================================
print("\n" + "=" * 60)
print("ğŸ§  ì‹ ê²½ë§ ë ˆì´ì–´ í…ŒìŠ¤íŠ¸")
print("=" * 60)

# Linear ë ˆì´ì–´
linear = L.Linear(10, in_size=5)
x = Variable(np.random.randn(4, 5).astype(np.float32))
y = linear(x)
print(f"\nLinear(5 â†’ 10)")
print(f"  ì…ë ¥: {x.shape} â†’ ì¶œë ¥: {y.shape}")

# Sequential ëª¨ë¸
model = Sequential(
    L.Linear(64),
    L.Linear(32),
    L.Linear(10)
)
x = Variable(np.random.randn(8, 100).astype(np.float32))
y = F.relu(model.layers[0](x))
y = F.relu(model.layers[1](y))
y = model.layers[2](y)
print(f"\nSequential(100 â†’ 64 â†’ 32 â†’ 10)")
print(f"  ì…ë ¥: (8, 100) â†’ ì¶œë ¥: {y.shape}")

# =============================================================================
# 4. Spiral ë°ì´í„°ì…‹ í•™ìŠµ í…ŒìŠ¤íŠ¸
# =============================================================================
print("\n" + "=" * 60)
print("ğŸŒ€ Spiral ë°ì´í„°ì…‹ í•™ìŠµ í…ŒìŠ¤íŠ¸")
print("=" * 60)

# í•˜ì´í¼íŒŒë¼ë¯¸í„°
max_epoch = 50
batch_size = 30
hidden_size = 10
lr = 1.0

# ë°ì´í„° ë¡œë“œ
train_set = Spiral(train=True)
test_set = Spiral(train=False)
train_loader = DataLoader(train_set, batch_size, shuffle=True)
test_loader = DataLoader(test_set, batch_size, shuffle=False)

print(f"\ní•™ìŠµ ë°ì´í„°: {len(train_set)}ê°œ")
print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_set)}ê°œ")

# ëª¨ë¸ & ì˜µí‹°ë§ˆì´ì €
model = MLP((hidden_size, 3), activation=F.relu)
optimizer = optimizers.SGD(lr).setup(model)

# í•™ìŠµ ë£¨í”„
print(f"\ní•™ìŠµ ì‹œì‘ (epochs: {max_epoch})")
print("-" * 40)

for epoch in range(max_epoch):
    sum_loss, sum_acc = 0, 0
    
    for x, t in train_loader:
        x = Variable(x)
        t = Variable(t)
        
        y = model(x)
        loss = F.softmax_cross_entropy(y, t)
        acc = F.accuracy(y, t)
        
        model.cleargrads()
        loss.backward()
        optimizer.update()
        
        sum_loss += float(loss.data) * len(t)
        sum_acc += float(acc.data) * len(t)
    
    avg_loss = sum_loss / len(train_set)
    avg_acc = sum_acc / len(train_set)
    
    if (epoch + 1) % 10 == 0:
        print(f"  Epoch {epoch+1:3d} | Loss: {avg_loss:.4f} | Acc: {avg_acc:.4f}")

# í…ŒìŠ¤íŠ¸
print("-" * 40)
print("í…ŒìŠ¤íŠ¸ ì¤‘...")

with dezero.no_grad():
    sum_acc = 0
    for x, t in test_loader:
        x = Variable(x)
        t = Variable(t)
        y = model(x)
        acc = F.accuracy(y, t)
        sum_acc += float(acc.data) * len(t)
    
    test_acc = sum_acc / len(test_set)
    print(f"í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc:.4f}")

# =============================================================================
# 5. ì˜µí‹°ë§ˆì´ì € ë¹„êµ í…ŒìŠ¤íŠ¸
# =============================================================================
print("\n" + "=" * 60)
print("âš¡ ì˜µí‹°ë§ˆì´ì € ë¹„êµ í…ŒìŠ¤íŠ¸")
print("=" * 60)

def train_with_optimizer(opt_class, opt_name, **kwargs):
    model = MLP((hidden_size, 3), activation=F.relu)
    optimizer = opt_class(**kwargs).setup(model)
    
    losses = []
    for epoch in range(30):
        sum_loss = 0
        for x, t in train_loader:
            x, t = Variable(x), Variable(t)
            y = model(x)
            loss = F.softmax_cross_entropy(y, t)
            model.cleargrads()
            loss.backward()
            optimizer.update()
            sum_loss += float(loss.data)
        losses.append(sum_loss / len(train_loader))
    
    print(f"  {opt_name:15s} | ìµœì¢… Loss: {losses[-1]:.4f}")
    return losses

print("\n30 ì—í­ í•™ìŠµ í›„ Loss ë¹„êµ:")
print("-" * 40)

train_with_optimizer(optimizers.SGD, "SGD", lr=1.0)
train_with_optimizer(optimizers.MomentumSGD, "MomentumSGD", lr=0.1)
train_with_optimizer(optimizers.AdaGrad, "AdaGrad", lr=0.1)
train_with_optimizer(optimizers.Adam, "Adam", alpha=0.01)

# =============================================================================
# 6. GPU (MLX) í…ŒìŠ¤íŠ¸ (ê°€ëŠ¥í•œ ê²½ìš°)
# =============================================================================
print("\n" + "=" * 60)
print("ğŸš€ GPU (MLX) í…ŒìŠ¤íŠ¸")
print("=" * 60)

if cuda.gpu_enable:
    import mlx.core as mx
    
    # NumPy â†’ MLX ë³€í™˜
    x_np = np.array([1.0, 2.0, 3.0], dtype=np.float32)
    x_mlx = cuda.as_gpu(x_np)
    
    print(f"\nNumPy ë°°ì—´: {x_np}")
    print(f"MLX ë°°ì—´: {x_mlx}")
    print(f"íƒ€ì…: {type(x_mlx)}")
    
    # MLXì—ì„œ ì—°ì‚°
    y_mlx = x_mlx ** 2 + x_mlx
    print(f"xÂ² + x = {y_mlx}")
    
    # ë‹¤ì‹œ NumPyë¡œ
    y_np = cuda.as_numpy(y_mlx)
    print(f"NumPyë¡œ ë³€í™˜: {y_np}")
    
    print("\nâœ“ MLX ë°±ì—”ë“œ ì •ìƒ ì‘ë™!")
else:
    print("\nâš  MLXê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
    print("  Apple Silicon Macì—ì„œ MLXë¥¼ ì„¤ì¹˜í•˜ë ¤ë©´:")
    print("  $ pip install mlx")

# =============================================================================
# ì™„ë£Œ
# =============================================================================
print("\n" + "=" * 60)
print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
print("=" * 60)
